{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a74911",
   "metadata": {},
   "source": [
    "# PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf9ac11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ac396a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/ynjn/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "022e9699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('young', 'JJ'),\n",
       " ('lady', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('walking', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('beach', 'NN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"A very beautiful young lady is walking on the beach\"\n",
    "# tokenizing words\n",
    "tokenized_words = word_tokenize(sentence)\n",
    "\n",
    "# words in tokenized_words\n",
    "tagged_words = nltk.pos_tag(tokenized_words)\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457d4fb",
   "metadata": {},
   "source": [
    "# Extracting noun phrase from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33a87013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  A/DT\n",
      "  very/RB\n",
      "  (NP beautiful/JJ young/JJ lady/NN)\n",
      "  is/VBZ\n",
      "  walking/VBG\n",
      "  on/IN\n",
      "  (NP the/DT beach/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"  # DT:한정사  JJ:형용사  NN:단일명사\n",
    "\n",
    "# create a parser\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "print(parser.parse(tagged_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc665ab",
   "metadata": {},
   "source": [
    "# Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c7ec520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT very/RB)\n",
      "  beautiful/JJ\n",
      "  young/JJ\n",
      "  (NP lady/NN is/VBZ walking/VBG on/IN the/DT beach/NN))\n"
     ]
    }
   ],
   "source": [
    "# excluding adjectives from the chunk\n",
    "grammar = r\"\"\" NP: {<.*>+}\n",
    "                }<JJ>+{\"\"\"\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "print(parser.parse(tagged_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0421f4a",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cff21d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mr./NNP)\n",
      "  (PERSON Smith/NNP)\n",
      "  made/VBD\n",
      "  a/DT\n",
      "  deal/NN\n",
      "  on/IN\n",
      "  a/DT\n",
      "  beach/NN\n",
      "  of/IN\n",
      "  (GPE Switzerland/NNP)\n",
      "  near/IN\n",
      "  (ORGANIZATION WHO/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Mr. Smith made a deal on a beach of Switzerland near WHO.\"\n",
    "tokenized_words = word_tokenize(sentence)\n",
    "tagged_words = nltk.pos_tag(tokenized_words)\n",
    "\n",
    "ner = nltk.ne_chunk(tagged_words,binary=False)\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995514ed",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abf6e76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('fun.n.01')\n",
      "Synset('fun.n.02')\n",
      "Synset('fun.n.03')\n",
      "Synset('playfulness.n.02')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "for words in wordnet.synsets(\"Fun\"):\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5f785ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun.n.01\n",
      "activities that are enjoyable or amusing\n",
      "['I do it for the fun of it', 'he is fun to have around']\n",
      "Lemma('fun.n.01.fun')\n",
      "Lemma('fun.n.01.merriment')\n",
      "Lemma('fun.n.01.playfulness')\n",
      "==================================================\n",
      "fun.n.02\n",
      "verbal wit or mockery (often at another's expense but not to be taken seriously)\n",
      "['he became a figure of fun', 'he said it in sport']\n",
      "Lemma('fun.n.02.fun')\n",
      "Lemma('fun.n.02.play')\n",
      "Lemma('fun.n.02.sport')\n",
      "==================================================\n",
      "fun.n.03\n",
      "violent and excited activity\n",
      "['she asked for money and then the fun began', 'they began to fight like fun']\n",
      "Lemma('fun.n.03.fun')\n",
      "==================================================\n",
      "playfulness.n.02\n",
      "a disposition to find (or make) causes for amusement\n",
      "['her playfulness surprised me', 'he was fun to be with']\n",
      "Lemma('playfulness.n.02.playfulness')\n",
      "Lemma('playfulness.n.02.fun')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# word meaning with definitions\n",
    "for words in wordnet.synsets(\"Fun\"):\n",
    "    print(words.name())\n",
    "    print(words.definition())\n",
    "    print(words.examples())\n",
    "    \n",
    "    for lemma in words.lemmas():\n",
    "        print(lemma)\n",
    "    print(\"\".join([\"=\"]*50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14297646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abadc9b7",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2abb9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb67f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Jim and Pam travelled by the bus:\",\n",
    "    \"The train was late\",\n",
    "    \"The flight was full.Travelling by flight is expensive\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d103526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# generate output for BOW\n",
    "bow = cv.fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc87d2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jim': 7, 'and': 0, 'pam': 9, 'travelled': 12, 'by': 2, 'the': 10, 'bus': 1, 'train': 11, 'was': 14, 'late': 8, 'flight': 4, 'full': 5, 'travelling': 13, 'is': 6, 'expensive': 3}\n"
     ]
    }
   ],
   "source": [
    "# total words with their index in model\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baecb2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'bus', 'by', 'expensive', 'flight', 'full', 'is', 'jim', 'late', 'pam', 'the', 'train', 'travelled', 'travelling', 'was']\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e83d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 0 0 1 0 1 1 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 1 0 0 1]\n",
      " [0 0 1 1 2 1 1 0 0 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a83d0",
   "metadata": {},
   "source": [
    "# TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10800707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c852b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentenses to analyze\n",
    "sentences = [\n",
    "    'This is the first document',\n",
    "    'This document is the second document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab78c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object\n",
    "vectorizer = TfidfVectorizer(norm=None) # norm{'l1', 'l2'}, default is 'l2'\n",
    "# generate output for TF_IDF:\n",
    "X = vectorizer.fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa24ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 5, 'is': 2, 'the': 4, 'first': 1, 'document': 0, 'second': 3}\n"
     ]
    }
   ],
   "source": [
    "# total words with their index in model\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10f3021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'second', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca367e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.40546511 1.         0.         1.         1.        ]\n",
      " [2.         0.         1.         1.40546511 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# the TF-IDF result\n",
    "# The higher the TF*IDF score, the rarer or unique or valuable the term and vice versa.\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa201ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
