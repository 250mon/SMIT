# -*- coding: utf-8 -*-
"""MNIST single layer network with TensorBoard.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cMF9eJtmf8J_sstPngmU6rBWXAfzapln

**[MST-01]** 모듈을 임포트하고 난수의 시드를 설정한다.
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data

np.random.seed(20160612)
tf.set_random_seed(20160612)

"""**[MST-02]** MNIST 데이터 세트를 준비한다."""

mnist = input_data.read_data_sets("./mnist_data/", one_hot=True)

"""**[MST-03]** 단층 신경망을 표현하는 클래스를 정의한다."""

class SingleLayerNetwork:
    def __init__(self, num_units):
        with tf.Graph().as_default():
            self.prepare_model(num_units)
            self.prepare_session()

    def prepare_model(self, num_units):
        with tf.name_scope('input'):
            x = tf.placeholder(tf.float32, [None, 784], name='input')
        
        with tf.name_scope('hidden'):
            w1 = tf.Variable(tf.truncated_normal([784, num_units]),
                             name='weights')        
            b1 = tf.Variable(tf.zeros([num_units]), name='biases')
            hidden1 = tf.nn.relu(tf.matmul(x, w1) + b1, name='hidden1')
        
        with tf.name_scope('output'):
            w0 = tf.Variable(tf.zeros([num_units, 10]), name='weights')
            b0 = tf.Variable(tf.zeros([10]), name='biases')
            p = tf.nn.softmax(tf.matmul(hidden1, w0) + b0, name='softmax')

        with tf.name_scope('optimizer'):
            t = tf.placeholder(tf.float32, [None, 10], name='labels')
            #loss = -tf.reduce_sum(t * tf.log(p), name='loss')
            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=p, labels=tf.stop_gradient(t)), name='loss')
            train_step = tf.train.AdamOptimizer().minimize(loss)

        with tf.name_scope('evaluator'):
            correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))
            accuracy = tf.reduce_mean(tf.cast(correct_prediction,
                                              tf.float32), name='accuracy')

        # tf.scalar_summary("loss", loss)
        tf.summary.scalar("loss", loss)
        tf.summary.scalar("accuracy", accuracy)
        #tf.histogram_summary("weights_hidden", w1)
        tf.summary.histogram("weights_hidden", w1)
        tf.summary.histogram("biases_hidden", b1)
        tf.summary.histogram("weights_output", w0)
        tf.summary.histogram("biases_output", b0)
                
        self.x, self.t, self.p = x, t, p
        self.train_step = train_step
        self.loss = loss
        self.accuracy = accuracy
            
    def prepare_session(self):
        sess = tf.InteractiveSession()
        sess.run(tf.initialize_all_variables())
        summary = tf.summary.merge_all()     
        writer = tf.summary.FileWriter("./mnist_sl_logs", sess.graph)
        
        self.sess = sess
        self.summary = summary
        self.writer = writer

"""**[MST-04]** 텐서보드용 데이터 출력 디렉토리를 삭제해서 초기화해둔다."""



"""**[MST-05]** 파라미터 최적화를 2000회 반복한다. 테스트 세트에 대해 약 97%의 정답률을 얻을 수 있다."""

# Commented out IPython magic to ensure Python compatibility.
nn = SingleLayerNetwork(1024)

batch_size = 100
num_epochs = 50
num_iterations = int(mnist.train.num_examples / batch_size)
print("num_iterations = %d" %(num_iterations))

epoch = 0
for _ in range(num_epochs):
    epoch += 1
    avg_loss = 0 
    avg_accuracy = 0
    
    for iteration in range (num_iterations):
      batch_xs, batch_ts = mnist.train.next_batch(batch_size)
      _, loss_val, acc_val = nn.sess.run([nn.train_step, nn.loss, nn.accuracy], feed_dict={nn.x: batch_xs, nn.t: batch_ts})
      avg_loss += loss_val / num_iterations
      avg_accuracy += acc_val / num_iterations
      
    summary, loss_val, acc_val = nn.sess.run([nn.summary, nn.loss, nn.accuracy],
                                            feed_dict={nn.x:mnist.test.images, nn.t: mnist.test.labels})
    print("Epoch : %04d,  train_Loss: %0.9f,  train_Accuracy: %0.9f  test_Loss: %0.9f,  test_Accuracy: %0.9f" 
                      %(epoch, avg_loss, avg_accuracy, loss_val, acc_val))
    nn.writer.add_summary(summary, epoch)
  
print("Learning Finished")
  


